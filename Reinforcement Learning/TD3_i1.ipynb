{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "#Actor network\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, action_dim)\n",
    "        self.max_action = max_action\n",
    "\n",
    "\n",
    "    def forward(self, state):\n",
    "        state = state.to(torch.device(\"cuda\"))\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return torch.tanh(self.fc3(x)) * self.max_action \n",
    "\n",
    "#Critic network\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "\n",
    "        self.fc4 = nn.Linear(state_dim + action_dim, 256)\n",
    "        self.fc5 = nn.Linear(256, 256)\n",
    "        self.fc6 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        state_action = torch.cat([state, action], dim=1).to(torch.device(\"cuda\"))\n",
    "\n",
    "        q1 = F.relu(self.fc1(state_action))\n",
    "        q1 = F.relu(self.fc2(q1))\n",
    "        q1 = self.fc3(q1)\n",
    "\n",
    "        q2 = F.relu(self.fc4(state_action))\n",
    "        q2 = F.relu(self.fc5(q2))\n",
    "        q2 = self.fc6(q2)\n",
    "\n",
    "        return q1, q2\n",
    "\n",
    "# TD3 Agent\n",
    "class TD3Agent:\n",
    "    def __init__(self, state_dim, action_dim, max_action, gamma=0.99, tau=0.005, actor_lr=3e-4, critic_lr=3e-4):\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.max_action = max_action\n",
    "\n",
    "        self.actor = Actor(state_dim, action_dim, max_action).to(torch.device(\"cuda\"))\n",
    "        self.actor_target = Actor(state_dim, action_dim, max_action).to(torch.device(\"cuda\"))\n",
    "        self.critic = Critic(state_dim, action_dim).to(torch.device(\"cuda\"))\n",
    "        self.critic_target = Critic(state_dim, action_dim).to(torch.device(\"cuda\"))\n",
    "\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "\n",
    "        self.replay_buffer = []\n",
    "        self.buffer_capacity = 2000000  \n",
    "        self.batch_size = 128  \n",
    "        self.policy_delay = 2\n",
    "        self.policy_update_step = 0\n",
    "\n",
    "        self.exploration_noise = 0.3 \n",
    "\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(torch.device(\"cuda\"))\n",
    "        action = self.actor(state).cpu().data.numpy().flatten()\n",
    "\n",
    "\n",
    "        if random.random() < 0.1:  \n",
    "            action += np.array([0.5, 0.5, 0.5])  \n",
    "\n",
    "        noise = np.random.normal(0, self.exploration_noise * self.max_action, size=action.shape)\n",
    "        action = np.clip(action + noise, -self.max_action, self.max_action)\n",
    "\n",
    "        # Decay noise over time\n",
    "        self.exploration_noise = max(0.1, self.exploration_noise * 0.999)\n",
    "\n",
    "        return action\n",
    "\n",
    "\n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        if len(self.replay_buffer) >= self.buffer_capacity:\n",
    "            self.replay_buffer.pop(0)\n",
    "        self.replay_buffer.append((state, action, reward, next_state, float(done)))\n",
    "\n",
    "    def train(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = random.sample(self.replay_buffer, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.FloatTensor(states).to(torch.device(\"cuda\"))\n",
    "        actions = torch.FloatTensor(actions).to(torch.device(\"cuda\"))\n",
    "        rewards = torch.FloatTensor(rewards).unsqueeze(1).to(torch.device(\"cuda\"))\n",
    "        next_states = torch.FloatTensor(next_states).to(torch.device(\"cuda\"))\n",
    "        dones = torch.FloatTensor(dones).unsqueeze(1).to(torch.device(\"cuda\"))\n",
    "\n",
    "        next_actions = (self.actor_target(next_states)).clamp(-self.max_action, self.max_action)\n",
    "\n",
    "        q1_target, q2_target = self.critic_target(next_states, next_actions)\n",
    "        q_target = rewards + (1 - dones) * self.gamma * torch.min(q1_target, q2_target).detach()\n",
    "\n",
    "        q1, q2 = self.critic(states, actions)\n",
    "        critic_loss = F.mse_loss(q1, q_target) + F.mse_loss(q2, q_target)\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        if self.policy_update_step % self.policy_delay == 0:\n",
    "            actor_loss = -self.critic(states, self.actor(states))[0].mean()\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "\n",
    "            self._soft_update(self.actor, self.actor_target)\n",
    "            self._soft_update(self.critic, self.critic_target)\n",
    "\n",
    "        self.policy_update_step += 1\n",
    "\n",
    "    def _soft_update(self, local_model, target_model):\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(self.tau * local_param.data + (1.0 - self.tau) * target_param.data)\n",
    "\n",
    "# Custom Reward Function\n",
    "def custom_reward(state, next_state, action):\n",
    "    velocity_x = next_state[5]  # Forward velocity\n",
    "    height = next_state[0]  # Hopper height\n",
    "    torso_angle = next_state[1]  # Torso angle\n",
    "    hip_angle = next_state[2]  \n",
    "    knee_angle = next_state[3]  \n",
    "    ankle_angle = next_state[4]\n",
    "    hinge_angle = next_state[8]  \n",
    "    torso_angular_velocity = next_state[6]  # Angular velocity of torso\n",
    "    \n",
    "\n",
    "    energy_penalty = -0.01 * np.square(action).sum()\n",
    "    forward_reward = 8.0 * velocity_x  \n",
    "    jump_reward = 10.0 * (height - 0.5) + 5.0 * np.exp(-5 * (height - 1.2)**2) \n",
    "    hinge_extension_reward = 2.5 * (30 - hinge_angle) \n",
    "    height_penalty = -300.0 if height < 0.4 else -50.0 if height < 0.5 else 0.0  \n",
    "    torso_penalty = -100.0 * abs(torso_angle) if abs(torso_angle) > 0.4 else 0.0  \n",
    "    angular_velocity_penalty = -20.0 * abs(torso_angular_velocity)\n",
    "    knee_movement_reward = 15.0 * abs(knee_angle - 0.6)  \n",
    "    hip_penalty = -40.0 * max(0, abs(hip_angle) - 1.0)  \n",
    "    knee_penalty = -40.0 * max(0, abs(knee_angle) - 1.2)  \n",
    "    ankle_penalty = -40.0 * max(0, abs(ankle_angle) - 0.8)  \n",
    "\n",
    "    # Final Reward Calculation\n",
    "    return (forward_reward + jump_reward + hinge_extension_reward + height_penalty +\n",
    "            torso_penalty + angular_velocity_penalty + hip_penalty + knee_penalty + \n",
    "            ankle_penalty + energy_penalty + knee_movement_reward)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Environment Setup\n",
    "env = gym.make(\n",
    "    \"Hopper-v4\",\n",
    "    render_mode=\"human\",\n",
    "    forward_reward_weight=10.0,  # Increase forward movement incentive\n",
    "    ctrl_cost_weight=0.005,  # Reduce action penalty to encourage bigger movements\n",
    "    healthy_reward=2.0,  # Small bonus for staying upright\n",
    "    terminate_when_unhealthy=False,  # End episode if it falls\n",
    ")\n",
    "\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])\n",
    "agent = TD3Agent(state_dim, action_dim, max_action)\n",
    "\n",
    "# Training\n",
    "num_episodes = 10000\n",
    "save_interval = num_episodes // 10  # Save every 10% of training\n",
    "for episode in tqdm(range(num_episodes)):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = agent.select_action(state)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        reward = custom_reward(state, next_state, action)\n",
    "        done = terminated or truncated or (next_state[5] < 0.1 and next_state[0] < 0.5)  # Reset if stopped or fallen\n",
    "        agent.store_transition(state, action, reward, next_state, done)\n",
    "        agent.train()\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "    print(f\"Episode {episode}, Reward: {episode_reward:.2f}\")\n",
    "    if (episode + 1) % save_interval == 0:\n",
    "        torch.save(agent.actor.state_dict(), f\"td3_actor_{episode+1}.pth\")\n",
    "        torch.save(agent.critic.state_dict(), f\"td3_critic_{episode+1}.pth\")\n",
    "        print(f\"Model saved at episode {episode+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import importlib\n",
    "\n",
    "spec = importlib.util.spec_from_file_location(\"Actor\", r\"C:\\Users\\______\\Downloads\\CODE\\Custom_reward_td3.py\")\n",
    "module = importlib.util.module_from_spec(spec)\n",
    "sys.modules[\"Actor\"] = module\n",
    "spec.loader.exec_module(module)\n",
    "Actor = module.Actor\n",
    "\n",
    "# Load environment\n",
    "env = gym.make(\n",
    "    \"Hopper-v5\",\n",
    "    render_mode=\"human\",\n",
    "    forward_reward_weight=10.0,\n",
    "    ctrl_cost_weight=0.005,\n",
    "    healthy_reward=2.0,\n",
    "    terminate_when_unhealthy=False,\n",
    ")\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])\n",
    "\n",
    "# Load trained actor\n",
    "actor = Actor(state_dim, action_dim, max_action).to(torch.device(\"cuda\"))\n",
    "actor.load_state_dict(torch.load(r\"C:\\Users\\_____\\Downloads\\results\\td3_actor_1000.pth\")) \n",
    "actor.eval()\n",
    "\n",
    "\n",
    "num_test_episodes = 10  \n",
    "total_rewards = []\n",
    "\n",
    "for episode in tqdm(range(num_test_episodes)):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(torch.device(\"cuda\"))\n",
    "        action = actor(state).cpu().data.numpy().flatten()\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        env.render()\n",
    "\n",
    "    total_rewards.append(episode_reward)\n",
    "    print(f\"Episode {episode+1}: Reward = {episode_reward:.2f}\")\n",
    "\n",
    "# Compute and display average performance\n",
    "avg_reward = np.mean(total_rewards)\n",
    "print(f\"\\nAverage Reward over {num_test_episodes} episodes: {avg_reward:.2f}\")\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
